{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"hadoop_install.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"myPIGP-mwKBD"},"source":["#Running Hadoop MapReduce On Google Colab\n","\n","In our previous tutorial we went through the steps of installing a single-node, pseude-distributed Hadoop cluster on Google Colab. This tutorial will show you how to run a simple map-reduce job on Hadoop. \n","\n","The tutorial should not take note take more thant 30 minutes. You do not have to complete it in one sitting.  \n","\n","The tutorial has been written in a way such that all of the commands work out of the box in Google Colab. However, if a particular command does not work you get a weird error message, please add your question to the discussion forum.\n","\n","The main steps for running a MapReduce Hadoop jobs are listed below.\n","\n","1. [Hadoop Install](#hadoop)\n","2. [Running WordCount](#wordcount)\n","3. [Conclusion](#end)\n"]},{"cell_type":"markdown","metadata":{"id":"j9bT9M1yvyXG"},"source":["## <a name=\"hadoop\"></a>Hadoop Install\n","Since the Google Colab environment is refreshed each time you open a new notebook, we will first need to install Hadoop in this VM instance. You can just follow the steps in the previous tutorial. For convenience the sequence of commands for Hadoop installation is given below. "]},{"cell_type":"code","metadata":{"id":"bijZAdD_cBMK"},"source":["!wget https://dlcdn.apache.org/hadoop/common/hadoop-3.3.1/hadoop-3.3.1.tar.gz"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nVce513-cBHm"},"source":["!tar -xzvf hadoop-3.3.1.tar.gz"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!ls hadoop-3.3.1/bin"],"metadata":{"id":"83DbRdnJHpR0"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JF-ze-YOdync"},"source":["!cp -r hadoop-3.3.1/ /usr/local/"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_OUc19ZtcBG5"},"source":["!readlink -f /usr/bin/java "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now, use the folder navigation pane on the left to browse to the file `usr/local/hadoop-3.3.0/etc/hadoop/hadoop-env.sh`. Double-click on the file to open it for editing. Uncomment the line begins with `export JAVA_HOME=` (should be line 54). Then add the Java path after the `=`\n","\n","```bash\n","export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64/\n","```"],"metadata":{"id":"OEwDUJaWcVKc"}},{"cell_type":"code","source":["!/usr/local/hadoop-3.3.1/bin/hdfs namenode -format"],"metadata":{"id":"FkKMT--cN01v"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Zhf-zK7NcBDF"},"source":["!/usr/local/hadoop-3.3.1/bin/hadoop"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##<a name=\"end\"></a>The Word Count Problem \n","\n","The word count application counts the number occurrences of all words appearing in a set of documents. As discussed in the previous lectures, the word count problem is a classic example of a MapReduce task. In this tutorial we will run a simple MapReduce implementation of a word count application. \n","\n","### Getting the Code \n","\n","You can find a simple Python implementation from the course git repo. To obtain the code clone the repo to Google Colab  "],"metadata":{"id":"MMh5I6cWSrDv"}},{"cell_type":"code","source":["!git clone https://github.com/anjalysam/Hadoop.git "],"metadata":{"id":"DT9C-5zVT5aq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The mapper and reducer codes can be found in the `Hadoop/mapper.py` and `Hadoop/reducer.py` files. For convenience, let's copy them to the current directory. "],"metadata":{"id":"_jGVp6TuUHVS"}},{"cell_type":"code","source":["!cp Hadoop/mapper.py ."],"metadata":{"id":"iNbrL7Lmgb8b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!cp Hadoop/reducer.py ."],"metadata":{"id":"v7WDIwQ5gm1i"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["To be able to execute these codes, we will need to set the execute permission. "],"metadata":{"id":"ZyvRcSuFV0Bm"}},{"cell_type":"code","source":["!chmod u+x mapper.py\n","!chmod u+rwx reducer.py"],"metadata":{"id":"RggUUHNfgsIR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Input and Output \n","\n","For any Hadoop job, we need to an input and output directory. These directories can be given any names. By convention, the names typically contain input/outpu as a suffix/prefix. Let's create the input directory. "],"metadata":{"id":"LZIPAfyAO5Ap"}},{"cell_type":"code","metadata":{"id":"uI-YBPIzcBCA"},"source":["!mkdir ~/input"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The output directory will be automatically created when the Hadoop job runs. "],"metadata":{"id":"R2xVEFgWSA9X"}},{"cell_type":"markdown","source":["### Obtaining the input. \n","\n","We will run the word count problem on datasets from newsgroups. You can fetch the datasets using the following command"],"metadata":{"id":"E6IlYraCSlmh"}},{"cell_type":"code","source":["!wget http://qwone.com/~jason/20Newsgroups/20news-18828.tar.gz"],"metadata":{"id":"GvBOgTvvdUUq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The dataset is in gzipped format. Let's untar and unzip it. "],"metadata":{"id":"rSnavduZVGRH"}},{"cell_type":"code","source":["!tar -xzvf 20news-18828.tar.gz"],"metadata":{"id":"6gCx4WFZe-aD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now, lets copy one of the files to the input directory."],"metadata":{"id":"o6cL9mWEbkcS"}},{"cell_type":"code","source":["!cp 20news-18828/alt.atheism/49960 ~/input"],"metadata":{"id":"vXG_xB_ibron"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Running the job\n","\n","Now we are all set to run our first MapReduce Hadoop job. \n","\n","The general format for the command for running a MapReduce Haddop job is as follows: \n","\n","```bash \n","hadoop jar hadoop-streaming.jar \\\n","-input name_of_input_file \\\n","-output name_of_output_directory \\\n","-file name_of_mapper_file \\\n","-mapper the_mapper_cmd \\\n","-file name_of_reducer_file \\\n","-reducer the_reducer_cmd \\\n","```\n","\n","That's a pretty long command. Let's break it down. \n","\n","  * `hadoop` says that we are launching a Hadoop task \n","  * `jar` is the command to execute a Java program.\n","  * `hadoop-streaming.jar` is a utility that comes with Haddop; it converts mapper and reducer code written in a differen programming to run in a MapReduce pattern in Hadoop; in this example we are feeding Python but we could also have written the coee Ruby, R etc. \n","  * `-input name_of_input_file` specifies the name of the input file for the MapReduce application \n","  * `-output name_of_output_directory` is the output directory \n","  * `-file name_of_mapper_file` is the name of the mapper file \n","  * `-file name_of_reducer_file` is the name of the reducer file \n","  * `-mapper the_mapper_cmd` is the actual command that needs to be executed to run the mapper \n","  * `-reducer the_reducer_cmd` is the actual command that needs to be executed to run the reducer\n","\n","Our actual command for running the job is as follows"],"metadata":{"id":"lItkk4TLW24E"}},{"cell_type":"code","source":["!ls"],"metadata":{"id":"UlCgZvOTcriE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!/usr/local/hadoop-3.3.1/bin/hadoop jar /usr/local/hadoop-3.3.1/share/hadoop/tools/lib/hadoop-streaming-3.3.1.jar -input ~/input/49960 -output output -file mapper.py  -file reducer.py  -mapper 'python mapper.py'  -reducer 'python reducer.py'"],"metadata":{"id":"aYShAt24g-P-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["If everything went right, you should see a whole bunch of output. The last line should give you the name of the output directory. \n","\n","Let's check the contents of the output directory. "],"metadata":{"id":"Wzt7xscjVDsI"}},{"cell_type":"code","metadata":{"id":"mtr0xWbfcA5J"},"source":["!ls -ltr output"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["`part-00000` is the file that contains the output of the program. Let's check the contents of that file. "],"metadata":{"id":"4RMjutnYd0dF"}},{"cell_type":"code","source":["!cat output/part-00000"],"metadata":{"id":"WrAS6cQTfClZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## <a name=\"end\"></a>Conclusion\n","\n","That's it! You have run your first MapReduce job on Hadoop."],"metadata":{"id":"LCYWXrHfelrd"}},{"cell_type":"code","source":[""],"metadata":{"id":"X997-zykenQQ"},"execution_count":null,"outputs":[]}]}